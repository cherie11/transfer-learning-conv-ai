{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "dataset = json.load(open(\"data/my_dataset_next_cluster.json\",\"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/xw_wangcs/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk==3.5 in /home/xw_wangcs/anaconda3/lib/python3.7/site-packages (3.5)\n",
      "Requirement already satisfied: tqdm in /home/xw_wangcs/anaconda3/lib/python3.7/site-packages (from nltk==3.5) (4.31.1)\n",
      "Requirement already satisfied: joblib in /home/xw_wangcs/anaconda3/lib/python3.7/site-packages (from nltk==3.5) (0.14.1)\n",
      "Requirement already satisfied: click in /home/xw_wangcs/anaconda3/lib/python3.7/site-packages (from nltk==3.5) (7.0)\n",
      "Requirement already satisfied: regex in /home/xw_wangcs/anaconda3/lib/python3.7/site-packages (from nltk==3.5) (2020.5.13)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk==3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i',\n",
       "  'am',\n",
       "  'well',\n",
       "  'an',\n",
       "  'loving',\n",
       "  'this',\n",
       "  'interaction',\n",
       "  'how',\n",
       "  'are',\n",
       "  'you',\n",
       "  '?'],\n",
       " ['i',\n",
       "  'am',\n",
       "  'great',\n",
       "  '.',\n",
       "  'i',\n",
       "  'just',\n",
       "  'got',\n",
       "  'back',\n",
       "  'from',\n",
       "  'the',\n",
       "  'club',\n",
       "  '.'],\n",
       " ['this',\n",
       "  'is',\n",
       "  'my',\n",
       "  'favorite',\n",
       "  'time',\n",
       "  'of',\n",
       "  'the',\n",
       "  'year',\n",
       "  'season',\n",
       "  'wise'],\n",
       " ['i',\n",
       "  'would',\n",
       "  'rather',\n",
       "  'eat',\n",
       "  'chocolate',\n",
       "  'cake',\n",
       "  'during',\n",
       "  'this',\n",
       "  'season',\n",
       "  '.'],\n",
       " ['what',\n",
       "  'club',\n",
       "  'did',\n",
       "  'you',\n",
       "  'go',\n",
       "  'to',\n",
       "  '?',\n",
       "  'me',\n",
       "  'an',\n",
       "  'timothy',\n",
       "  'watched',\n",
       "  'tv']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "preds=[]\n",
    "targets = []\n",
    "for dialog in dataset:\n",
    "    for utt in dialog['utterances']:\n",
    "#         if utt['pid']==1:\n",
    "#             continue\n",
    "        preds.append(word_tokenize(utt['pred']))\n",
    "        targets.append(word_tokenize(utt['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(25):\n",
    "    print('cluster',i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.017990043246505077\n",
      "2 0.12251778917756082\n",
      "3 0.2855790373808942\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "from functools import reduce \n",
    "def dist(n=3):\n",
    "    res = map(lambda sentence:list(ngrams(sentence, n)),preds)\n",
    "    res = reduce(lambda x,y:x+y,res)\n",
    "\n",
    "    print(n,1.0*len(set(list(res)))/len(res))\n",
    "\n",
    "dist(1)\n",
    "dist(2)\n",
    "dist(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.translate.nist_score as nist_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4613017270032969"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nist_score.corpus_nist(targets,preds,n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu1 0.16989282520811555\n"
     ]
    }
   ],
   "source": [
    "print(\"bleu1\",nltk.translate.bleu_score.corpus_bleu(targets,preds,weights=(1,0,0,0)))\n",
    "# print(\"bleu2\",nltk.translate.bleu_score.corpus_bleu(targets,preds,weights=(0,1,0,0)))\n",
    "# print(\"bleu3\",nltk.translate.bleu_score.corpus_bleu(targets,preds,weights=(0,0,1,0)))\n",
    "# print(\"bleu4\",nltk.translate.bleu_score.corpus_bleu(targets,preds,weights=(0,0,0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=[]\n",
    "targets = []\n",
    "for dialog in dataset:\n",
    "    for utt in dialog['utterances']:\n",
    "#         if utt['pid']==1:\n",
    "#             continue\n",
    "        preds.append(utt['pred'])\n",
    "        targets.append(utt['target'])\n",
    "with open('target.txt','w') as f:\n",
    "    f.write('\\n'.join(targets))\n",
    "with open('pred.txt','w') as f:\n",
    "    f.write('\\n'.join(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00016670632046768846"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.translate.bleu_score.corpus_bleu(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0,\n",
       " [0.16843367486257702, 0.0, 0.0, 0.0],\n",
       " 1.0,\n",
       " 10.693030341500572,\n",
       " 84229,\n",
       " 7877)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_bleu(targets, preds,max_order=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Python implementation of BLEU and smooth-BLEU.\n",
    "\n",
    "This module provides a Python implementation of BLEU and smooth-BLEU.\n",
    "Smooth BLEU is computed following the method outlined in the paper:\n",
    "Chin-Yew Lin, Franz Josef Och. ORANGE: a method for evaluating automatic\n",
    "evaluation metrics for machine translation. COLING 2004.\n",
    "\"\"\"\n",
    "\n",
    "import collections\n",
    "import math\n",
    "\n",
    "\n",
    "def _get_ngrams(segment, max_order):\n",
    "  \"\"\"Extracts all n-grams upto a given maximum order from an input segment.\n",
    "\n",
    "  Args:\n",
    "    segment: text segment from which n-grams will be extracted.\n",
    "    max_order: maximum length in tokens of the n-grams returned by this\n",
    "        methods.\n",
    "\n",
    "  Returns:\n",
    "    The Counter containing all n-grams upto max_order in segment\n",
    "    with a count of how many times each n-gram occurred.\n",
    "  \"\"\"\n",
    "  ngram_counts = collections.Counter()\n",
    "  for order in range(1, max_order + 1):\n",
    "    for i in range(0, len(segment) - order + 1):\n",
    "      ngram = tuple(segment[i:i+order])\n",
    "      ngram_counts[ngram] += 1\n",
    "  return ngram_counts\n",
    "\n",
    "\n",
    "def compute_bleu(reference_corpus, translation_corpus, max_order=4,\n",
    "                 smooth=False):\n",
    "  \"\"\"Computes BLEU score of translated segments against one or more references.\n",
    "\n",
    "  Args:\n",
    "    reference_corpus: list of lists of references for each translation. Each\n",
    "        reference should be tokenized into a list of tokens.\n",
    "    translation_corpus: list of translations to score. Each translation\n",
    "        should be tokenized into a list of tokens.\n",
    "    max_order: Maximum n-gram order to use when computing BLEU score.\n",
    "    smooth: Whether or not to apply Lin et al. 2004 smoothing.\n",
    "\n",
    "  Returns:\n",
    "    3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n",
    "    precisions and brevity penalty.\n",
    "  \"\"\"\n",
    "  matches_by_order = [0] * max_order\n",
    "  possible_matches_by_order = [0] * max_order\n",
    "  reference_length = 0\n",
    "  translation_length = 0\n",
    "  for (references, translation) in zip(reference_corpus,\n",
    "                                       translation_corpus):\n",
    "    reference_length += min(len(r) for r in references)\n",
    "    translation_length += len(translation)\n",
    "\n",
    "    merged_ref_ngram_counts = collections.Counter()\n",
    "    for reference in references:\n",
    "      merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n",
    "    translation_ngram_counts = _get_ngrams(translation, max_order)\n",
    "    overlap = translation_ngram_counts & merged_ref_ngram_counts\n",
    "    for ngram in overlap:\n",
    "      matches_by_order[len(ngram)-1] += overlap[ngram]\n",
    "    for order in range(1, max_order+1):\n",
    "      possible_matches = len(translation) - order + 1\n",
    "      if possible_matches > 0:\n",
    "        possible_matches_by_order[order-1] += possible_matches\n",
    "\n",
    "  precisions = [0] * max_order\n",
    "  for i in range(0, max_order):\n",
    "    if smooth:\n",
    "      precisions[i] = ((matches_by_order[i] + 1.) /\n",
    "                       (possible_matches_by_order[i] + 1.))\n",
    "    else:\n",
    "      if possible_matches_by_order[i] > 0:\n",
    "        precisions[i] = (float(matches_by_order[i]) /\n",
    "                         possible_matches_by_order[i])\n",
    "      else:\n",
    "        precisions[i] = 0.0\n",
    "\n",
    "  if min(precisions) > 0:\n",
    "    p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)\n",
    "    geo_mean = math.exp(p_log_sum)\n",
    "  else:\n",
    "    geo_mean = 0\n",
    "\n",
    "  ratio = float(translation_length) / reference_length\n",
    "\n",
    "  if ratio > 1.0:\n",
    "    bp = 1.\n",
    "  else:\n",
    "    bp = math.exp(1 - 1. / ratio)\n",
    "\n",
    "  bleu = geo_mean * bp\n",
    "\n",
    "  return (bleu, precisions, bp, ratio, translation_length, reference_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k,v in dataset.items():\n",
    "    dialogs = dataset[k]\n",
    "    for i in range(len(dialogs)):\n",
    "        for u in dialogs[i]['utterances']:\n",
    "            u['cluster'] = \"1\"\n",
    "            u['event'] = \"you are good\"\n",
    "    dataset[k] = dialogs\n",
    "dataset['train'][0]['utterances'][0]['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(dataset,open(\"data/personachat_self_original.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7801"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_datasets[\"valid\"][0].size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tensor_datasets[\"valid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'valid'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "dataset = json.load(open(\"/home/xw_wangcs/transfer-learning-conv-ai/data/my_dataset_next_cluster.json\",'r'))#/personachat_self_original.json\",\"r\"))\n",
    "dataset.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_dataset={}\n",
    "\n",
    "# for k,v in dataset.items():\n",
    "#     new_dataset[k] = []\n",
    "#     for dialog in dataset[k]:\n",
    "#         new_dialog = {}\n",
    "#         new_dialog['utterances']=[]\n",
    "#         for utter in  dialog['utterances']:\n",
    "#             if utter['id']==2:\n",
    "#                 new_dialog['utterances'].append(utter)\n",
    "#         new_dataset[k].append(new_dialog)\n",
    "# import json\n",
    "# json.dump(new_dataset,open(\"/home/xw_wangcs/transfer-learning-conv-ai/data/my_dataset_next_cluster_self.json\",'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 3374\n",
      "5 2994\n",
      "11 2051\n",
      "8 1534\n",
      "0 1930\n",
      "15 1635\n",
      "7 1234\n",
      "18 739\n",
      "6 2017\n",
      "3 2378\n",
      "2 2159\n",
      "1 2437\n",
      "4 2204\n",
      "20 997\n",
      "21 2541\n",
      "9 2209\n",
      "16 1683\n",
      "23 589\n",
      "14 988\n",
      "19 1517\n",
      "24 1162\n",
      "22 795\n",
      "12 800\n",
      "17 939\n",
      "13 1543\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "dataset = json.load(open(\"data/my_dataset_next_cluster.json\",'r'))#/personachat_self_original.json\",\"r\"))\n",
    "# dataset.keys()\n",
    "\n",
    "cluster_event={}\n",
    "\n",
    "for k,v in dataset.items():\n",
    "    for dialog in dataset[k]:\n",
    "\n",
    "        for utter in  dialog['utterances']:\n",
    "            c_id = utter['cluster']\n",
    "            if c_id not in cluster_event:\n",
    "            \n",
    "                cluster_event[c_id]=set()\n",
    "            if len(utter['event'])==0:\n",
    "                continue\n",
    "            cluster_event[c_id].add(utter['event'])\n",
    "for k,v in cluster_event.items():\n",
    "    print(k,len(v))  \n",
    "    cluster_event[k] = list(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "dataset = json.load(open(\"data/test.json\",'r'))#/personachat_self_original.json\",\"r\"))\n",
    "\n",
    "new_dataset = []\n",
    "for dialog in dataset:\n",
    "    new_utter = {}\n",
    "    new_utter['utterances'] = []\n",
    "    for utterance in dialog['utterances']:\n",
    "        tmp_utterance = copy.deepcopy(utterance)\n",
    "        new_utter['utterances'].append(tmp_utterance)\n",
    "        for i in range(25):\n",
    "            tmp_utterance = copy.deepcopy(utterance)\n",
    "#             tmp_utterance['cluster'] = i\n",
    "            if tmp_utterance['cluster']==i:\n",
    "                continue\n",
    "            tmp_utterance['cluster']=i\n",
    "            tmp_utterance['event'] = random.choice(cluster_event[i])\n",
    "            new_utter['utterances'].append(tmp_utterance)\n",
    "    new_dataset.append(new_utter)\n",
    "    \n",
    "json.dump(new_dataset,open(\"/home/xw_wangcs/transfer-learning-conv-ai/data/my_dataset_test_switch_inter_cluster.json\",'w'))            \n",
    "        \n",
    "\n",
    "dataset = json.load(open(\"data/test.json\",'r'))#/personachat_self_original.json\",\"r\"))\n",
    "\n",
    "new_dataset = []\n",
    "for dialog in dataset:\n",
    "    new_utter = {}\n",
    "    new_utter['utterances'] = []\n",
    "    for utterance in dialog['utterances']:\n",
    "#         new_utter['utterances'].append(utterance)\n",
    "        tmp_utterance = copy.deepcopy(utterance)\n",
    "        new_utter['utterances'].append(tmp_utterance)\n",
    "        for i in range(24):\n",
    "            tmp_utterance = copy.deepcopy(utterance)\n",
    "            tmp_utterance['event'] = random.choice(cluster_event[tmp_utterance['cluster']])\n",
    "            \n",
    "            new_utter['utterances'].append(tmp_utterance)\n",
    "    new_dataset.append(new_utter)\n",
    "    \n",
    "json.dump(new_dataset,open(\"/home/xw_wangcs/transfer-learning-conv-ai/data/my_dataset_test_switch_intra_cluster.json\",'w'))            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "new_dataset = []\n",
    "for dialog in dataset[:5]:\n",
    "    new_utter = {}\n",
    "    new_utter['utterances'] = []\n",
    "    for utterance in dialog['utterances']:\n",
    "#         new_utter['utterances'].append(utterance)\n",
    "        for i in range(25):\n",
    "            tmp_utterance = copy.deepcopy(utterance)\n",
    "            tmp_utterance['cluster'] = i\n",
    "            new_utter['utterances'].append(tmp_utterance)\n",
    "    new_dataset.append(new_utter)\n",
    "    \n",
    "# json.dump(new_dataset,open(\"/home/xw_wangcs/transfer-learning-conv-ai/data/my_dataset_test_switch_cluster.json\",'w'))            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
