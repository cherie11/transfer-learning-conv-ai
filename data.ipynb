{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "dataset = json.load(open(\"pred_with_switch_cluster_100.json\",\"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/xw_wangcs/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk==3.5 in /home/xw_wangcs/anaconda3/lib/python3.7/site-packages (3.5)\n",
      "Requirement already satisfied: tqdm in /home/xw_wangcs/anaconda3/lib/python3.7/site-packages (from nltk==3.5) (4.31.1)\n",
      "Requirement already satisfied: joblib in /home/xw_wangcs/anaconda3/lib/python3.7/site-packages (from nltk==3.5) (0.14.1)\n",
      "Requirement already satisfied: click in /home/xw_wangcs/anaconda3/lib/python3.7/site-packages (from nltk==3.5) (7.0)\n",
      "Requirement already satisfied: regex in /home/xw_wangcs/anaconda3/lib/python3.7/site-packages (from nltk==3.5) (2020.5.13)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk==3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i',\n",
       "  'am',\n",
       "  'well',\n",
       "  'an',\n",
       "  'loving',\n",
       "  'this',\n",
       "  'interaction',\n",
       "  'how',\n",
       "  'are',\n",
       "  'you',\n",
       "  '?'],\n",
       " ['i',\n",
       "  'am',\n",
       "  'great',\n",
       "  '.',\n",
       "  'i',\n",
       "  'just',\n",
       "  'got',\n",
       "  'back',\n",
       "  'from',\n",
       "  'the',\n",
       "  'club',\n",
       "  '.'],\n",
       " ['this',\n",
       "  'is',\n",
       "  'my',\n",
       "  'favorite',\n",
       "  'time',\n",
       "  'of',\n",
       "  'the',\n",
       "  'year',\n",
       "  'season',\n",
       "  'wise'],\n",
       " ['i',\n",
       "  'would',\n",
       "  'rather',\n",
       "  'eat',\n",
       "  'chocolate',\n",
       "  'cake',\n",
       "  'during',\n",
       "  'this',\n",
       "  'season',\n",
       "  '.'],\n",
       " ['what',\n",
       "  'club',\n",
       "  'did',\n",
       "  'you',\n",
       "  'go',\n",
       "  'to',\n",
       "  '?',\n",
       "  'me',\n",
       "  'an',\n",
       "  'timothy',\n",
       "  'watched',\n",
       "  'tv']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "preds=[]\n",
    "targets = []\n",
    "for dialog in dataset:\n",
    "    for utt in dialog['utterances']:\n",
    "#         if utt['pid']==1:\n",
    "#             continue\n",
    "        preds.append(word_tokenize(utt['pred']))\n",
    "        targets.append(word_tokenize(utt['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(25):\n",
    "    print('cluster',i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.017990043246505077\n",
      "2 0.12251778917756082\n",
      "3 0.2855790373808942\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "from functools import reduce \n",
    "def dist(n=3):\n",
    "    res = map(lambda sentence:list(ngrams(sentence, n)),preds)\n",
    "    res = reduce(lambda x,y:x+y,res)\n",
    "\n",
    "    print(n,1.0*len(set(list(res)))/len(res))\n",
    "\n",
    "dist(1)\n",
    "dist(2)\n",
    "dist(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.translate.nist_score as nist_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4613017270032969"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nist_score.corpus_nist(targets,preds,n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu1 0.16989282520811555\n"
     ]
    }
   ],
   "source": [
    "print(\"bleu1\",nltk.translate.bleu_score.corpus_bleu(targets,preds,weights=(1,0,0,0)))\n",
    "# print(\"bleu2\",nltk.translate.bleu_score.corpus_bleu(targets,preds,weights=(0,1,0,0)))\n",
    "# print(\"bleu3\",nltk.translate.bleu_score.corpus_bleu(targets,preds,weights=(0,0,1,0)))\n",
    "# print(\"bleu4\",nltk.translate.bleu_score.corpus_bleu(targets,preds,weights=(0,0,0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=[]\n",
    "targets = []\n",
    "for dialog in dataset:\n",
    "    for utt in dialog['utterances']:\n",
    "#         if utt['pid']==1:\n",
    "#             continue\n",
    "        preds.append(utt['pred'])\n",
    "        targets.append(utt['target'])\n",
    "with open('target.txt','w') as f:\n",
    "    f.write('\\n'.join(targets))\n",
    "with open('pred.txt','w') as f:\n",
    "    f.write('\\n'.join(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00016670632046768846"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.translate.bleu_score.corpus_bleu(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0,\n",
       " [0.16843367486257702, 0.0, 0.0, 0.0],\n",
       " 1.0,\n",
       " 10.693030341500572,\n",
       " 84229,\n",
       " 7877)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_bleu(targets, preds,max_order=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Python implementation of BLEU and smooth-BLEU.\n",
    "\n",
    "This module provides a Python implementation of BLEU and smooth-BLEU.\n",
    "Smooth BLEU is computed following the method outlined in the paper:\n",
    "Chin-Yew Lin, Franz Josef Och. ORANGE: a method for evaluating automatic\n",
    "evaluation metrics for machine translation. COLING 2004.\n",
    "\"\"\"\n",
    "\n",
    "import collections\n",
    "import math\n",
    "\n",
    "\n",
    "def _get_ngrams(segment, max_order):\n",
    "  \"\"\"Extracts all n-grams upto a given maximum order from an input segment.\n",
    "\n",
    "  Args:\n",
    "    segment: text segment from which n-grams will be extracted.\n",
    "    max_order: maximum length in tokens of the n-grams returned by this\n",
    "        methods.\n",
    "\n",
    "  Returns:\n",
    "    The Counter containing all n-grams upto max_order in segment\n",
    "    with a count of how many times each n-gram occurred.\n",
    "  \"\"\"\n",
    "  ngram_counts = collections.Counter()\n",
    "  for order in range(1, max_order + 1):\n",
    "    for i in range(0, len(segment) - order + 1):\n",
    "      ngram = tuple(segment[i:i+order])\n",
    "      ngram_counts[ngram] += 1\n",
    "  return ngram_counts\n",
    "\n",
    "\n",
    "def compute_bleu(reference_corpus, translation_corpus, max_order=4,\n",
    "                 smooth=False):\n",
    "  \"\"\"Computes BLEU score of translated segments against one or more references.\n",
    "\n",
    "  Args:\n",
    "    reference_corpus: list of lists of references for each translation. Each\n",
    "        reference should be tokenized into a list of tokens.\n",
    "    translation_corpus: list of translations to score. Each translation\n",
    "        should be tokenized into a list of tokens.\n",
    "    max_order: Maximum n-gram order to use when computing BLEU score.\n",
    "    smooth: Whether or not to apply Lin et al. 2004 smoothing.\n",
    "\n",
    "  Returns:\n",
    "    3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n",
    "    precisions and brevity penalty.\n",
    "  \"\"\"\n",
    "  matches_by_order = [0] * max_order\n",
    "  possible_matches_by_order = [0] * max_order\n",
    "  reference_length = 0\n",
    "  translation_length = 0\n",
    "  for (references, translation) in zip(reference_corpus,\n",
    "                                       translation_corpus):\n",
    "    reference_length += min(len(r) for r in references)\n",
    "    translation_length += len(translation)\n",
    "\n",
    "    merged_ref_ngram_counts = collections.Counter()\n",
    "    for reference in references:\n",
    "      merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n",
    "    translation_ngram_counts = _get_ngrams(translation, max_order)\n",
    "    overlap = translation_ngram_counts & merged_ref_ngram_counts\n",
    "    for ngram in overlap:\n",
    "      matches_by_order[len(ngram)-1] += overlap[ngram]\n",
    "    for order in range(1, max_order+1):\n",
    "      possible_matches = len(translation) - order + 1\n",
    "      if possible_matches > 0:\n",
    "        possible_matches_by_order[order-1] += possible_matches\n",
    "\n",
    "  precisions = [0] * max_order\n",
    "  for i in range(0, max_order):\n",
    "    if smooth:\n",
    "      precisions[i] = ((matches_by_order[i] + 1.) /\n",
    "                       (possible_matches_by_order[i] + 1.))\n",
    "    else:\n",
    "      if possible_matches_by_order[i] > 0:\n",
    "        precisions[i] = (float(matches_by_order[i]) /\n",
    "                         possible_matches_by_order[i])\n",
    "      else:\n",
    "        precisions[i] = 0.0\n",
    "\n",
    "  if min(precisions) > 0:\n",
    "    p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)\n",
    "    geo_mean = math.exp(p_log_sum)\n",
    "  else:\n",
    "    geo_mean = 0\n",
    "\n",
    "  ratio = float(translation_length) / reference_length\n",
    "\n",
    "  if ratio > 1.0:\n",
    "    bp = 1.\n",
    "  else:\n",
    "    bp = math.exp(1 - 1. / ratio)\n",
    "\n",
    "  bleu = geo_mean * bp\n",
    "\n",
    "  return (bleu, precisions, bp, ratio, translation_length, reference_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k,v in dataset.items():\n",
    "    dialogs = dataset[k]\n",
    "    for i in range(len(dialogs)):\n",
    "        for u in dialogs[i]['utterances']:\n",
    "            u['cluster'] = \"1\"\n",
    "            u['event'] = \"you are good\"\n",
    "    dataset[k] = dialogs\n",
    "dataset['train'][0]['utterances'][0]['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(dataset,open(\"data/personachat_self_original.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Running process -1\n",
      "INFO:root:Arguments: Namespace(dataset_cache='./dataset_cache', dataset_path='data/personachat_self_original.json', device='cuda', eval_before_start=False, f='/run/user/1009/jupyter/kernel-524f06c8-e39f-461d-9e77-7ec9edc75634.json', fp16='', gradient_accumulation_steps=8, lm_coef=1.0, local_rank=-1, lr=6.25e-05, max_history=2, max_norm=1.0, mc_coef=1.0, model_checkpoint='gpt2', n_epochs=3, num_candidates=2, personality_permutations=1, train_batch_size=4, valid_batch_size=4)\n",
      "INFO:root:Prepare tokenizer, pretrained model and optimizer.\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/xueweiwa/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/xueweiwa/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/xueweiwa/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942\n",
      "INFO:transformers.configuration_utils:Model config GPT2Config {\n",
      "  \"_num_labels\": 2,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /home/xueweiwa/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
      "INFO:transformers.modeling_utils:Weights of GPT2DoubleHeadsModel not initialized from pretrained model: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'multiple_choice_head.summary.bias', 'multiple_choice_head.summary.weight', 'lm_head.weight']\n",
      "INFO:transformers.tokenization_utils:Adding <bos> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Assigning <bos> to the bos_token key of the tokenizer\n",
      "INFO:transformers.tokenization_utils:Adding <eos> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Assigning <eos> to the eos_token key of the tokenizer\n",
      "INFO:transformers.tokenization_utils:Adding <pad> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Assigning <pad> to the pad_token key of the tokenizer\n",
      "INFO:transformers.tokenization_utils:Adding <speaker1> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Adding <speaker2> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Adding <xneed> to the vocabulary\n",
      "INFO:transformers.tokenization_utils:Assigning ['<speaker1>', '<speaker2>', '<xneed>'] to the additional_special_tokens key of the tokenizer\n",
      "INFO:root:Prepare datasets\n",
      "INFO:/home/xueweiwa/transfer-learning-conv-ai/utils.py:Load tokenized dataset from cache at ./dataset_cache_GPT2Tokenizer\n",
      "INFO:root:Build inputs and labels\n",
      "INFO:root:Pad inputs and convert to Tensor\n",
      "INFO:root:Build train and validation dataloaders\n",
      "INFO:root:Train dataset (Batch, Candidates, Seq length): torch.Size([131438, 2, 284])\n",
      "INFO:root:Valid dataset (Batch, Candidates, Seq length): torch.Size([7801, 20, 188])\n",
      "/home/xueweiwa/anaconda3/envs/py3/lib/python3.7/site-packages/ignite/handlers/checkpoint.py:369: UserWarning: Argument save_interval is deprecated and should be None. Please, use events filtering instead, e.g. Events.ITERATION_STARTED(every=1000)\n",
      "  warnings.warn(msg)\n",
      "INFO:ignite.engine.engine.Engine:Engine run starting with max_epochs=3.\n",
      "ERROR:ignite.engine.engine.Engine:Current run is terminating due to exception: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 7.92 GiB total capacity; 2.56 GiB already allocated; 26.00 MiB free; 2.65 GiB reserved in total by PyTorch).\n",
      "ERROR:ignite.engine.engine.Engine:Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 7.92 GiB total capacity; 2.56 GiB already allocated; 26.00 MiB free; 2.65 GiB reserved in total by PyTorch).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 7.92 GiB total capacity; 2.56 GiB already allocated; 26.00 MiB free; 2.65 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8197970ad239>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;31m# Run the training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;31m# On the main process: close tensorboard logger and rename the last checkpoint (for easy re-loading with OpenAIGPTModel.from_pretrained method)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.7/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.7/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Engine run is terminating due to exception: %s.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.7/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.7/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m                 \u001b[0mhours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch[%s] Complete. Time taken: %02d:%02d:%02d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.7/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Current run is terminating due to exception: %s.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.7/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.7/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-8197970ad239>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(engine, batch)\u001b[0m\n\u001b[1;32m    247\u001b[0m     (lm_loss), (mc_loss), *_ = model(\n\u001b[1;32m    248\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc_token_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmc_token_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mmc_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmc_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlm_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m     )\n\u001b[1;32m    251\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlm_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_coef\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmc_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmc_coef\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.7/site-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, mc_token_ids, lm_labels, mc_labels)\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m         )\n\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.7/site-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             outputs = block(\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m             )\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.7/site-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, layer_past, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.7/site-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1589\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m         \u001b[0msize_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1591\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msize_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 7.92 GiB total capacity; 2.56 GiB already allocated; 26.00 MiB free; 2.65 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# Copyright (c) 2019-present, HuggingFace Inc.\n",
    "# All rights reserved. This source code is licensed under the BSD-style license found in the LICENSE file in the root directory of this source tree.\n",
    "import os\n",
    "import math\n",
    "import logging\n",
    "from pprint import pformat\n",
    "from argparse import ArgumentParser\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.metrics import Accuracy, Loss, MetricsLambda, RunningAverage\n",
    "from ignite.contrib.handlers import ProgressBar, PiecewiseLinear\n",
    "from ignite.contrib.handlers.tensorboard_logger import TensorboardLogger, OutputHandler, OptimizerParamsHandler\n",
    "from transformers import (AdamW, OpenAIGPTDoubleHeadsModel, OpenAIGPTTokenizer,\n",
    "                          GPT2DoubleHeadsModel, GPT2Tokenizer, WEIGHTS_NAME, CONFIG_NAME)\n",
    "\n",
    "from utils import get_dataset, make_logdir\n",
    "\n",
    "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\", \"<xneed>\"]\n",
    "ATTR_TO_SPECIAL_TOKEN = {'bos_token': '<bos>', 'eos_token': '<eos>', 'pad_token': '<pad>',\n",
    "                         'additional_special_tokens': ['<speaker1>', '<speaker2>', '<xneed>']}\n",
    "\"\"\"\n",
    "\"input_ids\": PERSONA + CONTEXT + RESPONSE/DISTRACTOR\n",
    "\"mc_token_ids\":  len(seq) - 1\n",
    "\"lm_labels\": [-100,....., DISTRACTOR is -100/ RESPONSE is acture token shift 1]\n",
    "\"mc_labels\":  n_candidates -1\n",
    "\"cluster\" : clusterID\n",
    "\"event\" : event from atomic\n",
    "\n",
    "\"\"\"\n",
    "MODEL_INPUTS = [\"input_ids\", \"mc_token_ids\",\n",
    "                \"lm_labels\", \"mc_labels\", \"token_type_ids\",\"cluster\"]\n",
    "\n",
    "\n",
    "\n",
    "PADDED_INPUTS = [\"input_ids\", \"lm_labels\", \"token_type_ids\"]\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "def average_distributed_scalar(scalar, args):\n",
    "    \"\"\" Average a scalar over the nodes if we are in distributed training. We use this for distributed evaluation. \"\"\"\n",
    "    if args.local_rank == -1:\n",
    "        return scalar\n",
    "    scalar_t = torch.tensor(scalar, dtype=torch.float,\n",
    "                            device=args.device) / torch.distributed.get_world_size()\n",
    "    torch.distributed.all_reduce(scalar_t, op=torch.distributed.ReduceOp.SUM)\n",
    "    return scalar_t.item()\n",
    "\n",
    "\n",
    "def pad_dataset(dataset, padding=0):\n",
    "    \"\"\" Pad the dataset. This could be optimized by defining a Dataset class and padding at the batch level, but this is simpler. \"\"\"\n",
    "    max_l = max(len(x) for x in dataset[\"input_ids\"])\n",
    "    for name in PADDED_INPUTS:\n",
    "        dataset[name] = [x + [padding if name != \"lm_labels\" else -100]\n",
    "                         * (max_l - len(x)) for x in dataset[name]]\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def add_special_tokens_(model, tokenizer):\n",
    "    \"\"\" Add special tokens to the tokenizer and the model if they have not already been added. \"\"\"\n",
    "    orig_num_tokens = len(tokenizer.encoder)\n",
    "    num_added_tokens = tokenizer.add_special_tokens(\n",
    "        ATTR_TO_SPECIAL_TOKEN)  # doesn't add if they are already there\n",
    "    if num_added_tokens > 0:\n",
    "        model.resize_token_embeddings(\n",
    "            new_num_tokens=orig_num_tokens + num_added_tokens)\n",
    "\n",
    "\n",
    "def build_input_from_segments(persona, history, reply, tokenizer,cluster, event, lm_labels=False, with_eos=True):\n",
    "    \"\"\" Build a sequence of input from 3 segments: persona, history and last reply. \"\"\"\n",
    "    bos, eos, speaker1, speaker2, xneed = tokenizer.convert_tokens_to_ids(\n",
    "        SPECIAL_TOKENS[:-1])\n",
    "    sequence = [[bos] + list(chain(*persona)) + [xneed] + event] + \\\n",
    "        history + [reply + ([eos] if with_eos else [])]\n",
    "    # seq: <bos> PERSONA <sp1> U1P1 <sp2> U1P2 ... \n",
    "    sequence = [sequence[0]] + [[speaker2 if (len(sequence)-i) %\n",
    "                                 2 else speaker1] + s for i, s in enumerate(sequence[1:])]\n",
    "    instance = {}\n",
    "    instance[\"input_ids\"] = list(chain(*sequence))\n",
    "    instance[\"token_type_ids\"] = [speaker2 if i %\n",
    "                                  2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
    "    instance[\"mc_token_ids\"] = len(instance[\"input_ids\"]) - 1\n",
    "    instance['cluster'] = cluster\n",
    "    instance[\"lm_labels\"] = [-100] * len(instance[\"input_ids\"])\n",
    "    if lm_labels:\n",
    "        instance[\"lm_labels\"] = ([-100] * sum(len(s)\n",
    "                                              for s in sequence[:-1])) + [-100] + sequence[-1][1:]\n",
    "    return instance\n",
    "\n",
    "\n",
    "def get_data_loaders(args, tokenizer):\n",
    "    \"\"\" Prepare the dataset for training and evaluation \"\"\"\n",
    "    personachat = get_dataset(tokenizer, args.dataset_path, args.dataset_cache)\n",
    "\n",
    "    logger.info(\"Build inputs and labels\")\n",
    "    datasets = {\"train\": defaultdict(list), \"valid\": defaultdict(list)}\n",
    "    num_cluster = 25\n",
    "    for dataset_name, dataset in personachat.items():\n",
    "        num_candidates = len(dataset[0][\"utterances\"][0][\"candidates\"])\n",
    "        if args.num_candidates > 0 and dataset_name == 'train':\n",
    "            num_candidates = min(args.num_candidates, num_candidates)\n",
    "        for dialog in dataset:\n",
    "            persona = dialog[\"personality\"].copy()\n",
    "\n",
    "            for _ in range(args.personality_permutations):\n",
    "                for utterance in dialog[\"utterances\"]:\n",
    "                    cluster = utterance[\"cluster\"] \n",
    "                    event = utterance[\"event\"]\n",
    "                    history = utterance[\"history\"][-(2*args.max_history+1):]\n",
    "                    for j, candidate in enumerate(utterance[\"candidates\"][-num_candidates:]):\n",
    "                        lm_labels = bool(j == num_candidates-1)\n",
    "                        instance = build_input_from_segments(\n",
    "                            persona, history, candidate, tokenizer, cluster, event, lm_labels)\n",
    "                        for input_name, input_array in instance.items():\n",
    "                            datasets[dataset_name][input_name].append(\n",
    "                                input_array)\n",
    "                    datasets[dataset_name][\"mc_labels\"].append(\n",
    "                        num_candidates - 1)\n",
    "                    datasets[dataset_name][\"n_candidates\"] = num_candidates\n",
    "                    datasets[dataset_name][\"n_cluster\"] = num_cluster\n",
    "                # permuted personalities\n",
    "                persona = [persona[-1]] + persona[:-1]\n",
    "\n",
    "    logger.info(\"Pad inputs and convert to Tensor\")\n",
    "    tensor_datasets = {\"train\": [], \"valid\": []}\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        dataset = pad_dataset(\n",
    "            dataset, padding=tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-1]))\n",
    "        for input_name in MODEL_INPUTS:\n",
    "            tensor = torch.tensor(dataset[input_name])\n",
    "            if input_name != \"mc_labels\":\n",
    "                tensor = tensor.view(\n",
    "                    (-1, datasets[dataset_name][\"n_candidates\"]) + tensor.shape[1:])\n",
    "            tensor_datasets[dataset_name].append(tensor)\n",
    "\n",
    "    logger.info(\"Build train and validation dataloaders\")\n",
    "    train_dataset, valid_dataset = TensorDataset(\n",
    "        *tensor_datasets[\"train\"]), TensorDataset(*tensor_datasets[\"valid\"])\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        train_dataset) if args.distributed else None\n",
    "    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        valid_dataset) if args.distributed else None\n",
    "    train_loader = DataLoader(train_dataset, sampler=train_sampler,\n",
    "                              batch_size=args.train_batch_size, shuffle=(not args.distributed))\n",
    "    valid_loader = DataLoader(valid_dataset, sampler=valid_sampler,\n",
    "                              batch_size=args.valid_batch_size, shuffle=False)\n",
    "\n",
    "    logger.info(\"Train dataset (Batch, Candidates, Seq length): {}\".format(\n",
    "        train_dataset.tensors[0].shape))\n",
    "    logger.info(\"Valid dataset (Batch, Candidates, Seq length): {}\".format(\n",
    "        valid_dataset.tensors[0].shape))\n",
    "    return train_loader, valid_loader, train_sampler, valid_sampler\n",
    "\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"--dataset_path\", type=str, default=\"data/personachat_self_original.json\",\n",
    "                    help=\"Path or url of the dataset. If empty download from S3.\")\n",
    "parser.add_argument(\"-f\", type=str, default=\"data/personachat_self_original.json\",\n",
    "                    help=\"Path or url of the dataset. If empty download from S3.\")\n",
    "parser.add_argument(\"--dataset_cache\", type=str,\n",
    "                    default='./dataset_cache', help=\"Path or url of the dataset cache\")\n",
    "parser.add_argument(\"--model_checkpoint\", type=str,\n",
    "                    default=\"gpt2\", help=\"Path, url or short name of the model\")\n",
    "parser.add_argument(\"--num_candidates\", type=int, default=2,\n",
    "                    help=\"Number of candidates for training\")\n",
    "parser.add_argument(\"--max_history\", type=int, default=2,\n",
    "                    help=\"Number of previous exchanges to keep in history\")\n",
    "parser.add_argument(\"--train_batch_size\", type=int,\n",
    "                    default=4, help=\"Batch size for training\")\n",
    "parser.add_argument(\"--valid_batch_size\", type=int,\n",
    "                    default=4, help=\"Batch size for validation\")\n",
    "parser.add_argument(\"--gradient_accumulation_steps\", type=int,\n",
    "                    default=8, help=\"Accumulate gradients on several steps\")\n",
    "parser.add_argument(\"--lr\", type=float,\n",
    "                    default=6.25e-5, help=\"Learning rate\")\n",
    "parser.add_argument(\"--lm_coef\", type=float,\n",
    "                    default=1.0, help=\"LM loss coefficient\")\n",
    "parser.add_argument(\"--mc_coef\", type=float, default=1.0,\n",
    "                    help=\"Multiple-choice loss coefficient\")\n",
    "parser.add_argument(\"--max_norm\", type=float,\n",
    "                    default=1.0, help=\"Clipping gradient norm\")\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=3,\n",
    "                    help=\"Number of training epochs\")\n",
    "parser.add_argument(\"--personality_permutations\", type=int, default=1,\n",
    "                    help=\"Number of permutations of personality sentences\")\n",
    "parser.add_argument(\"--eval_before_start\", action='store_true',\n",
    "                    help=\"If true start with a first evaluation before training\")\n",
    "parser.add_argument(\"--device\", type=str, default=\"cuda\" if torch.cuda.is_available()\n",
    "                    else \"cpu\", help=\"Device (cuda or cpu)\")\n",
    "parser.add_argument(\"--fp16\", type=str, default=\"\",\n",
    "                    help=\"Set to O0, O1, O2 or O3 for fp16 training (see apex documentation)\")\n",
    "parser.add_argument(\"--local_rank\", type=int, default=-1,\n",
    "                    help=\"Local rank for distributed training (-1: not distributed)\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# logging is set to INFO (resp. WARN) for main (resp. auxiliary) process. logger.info => log main process only, logger.warning => log all processes\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
    "# This is a logger.warning: it will be printed by all distributed processes\n",
    "logger.warning(\"Running process %d\", args.local_rank)\n",
    "logger.info(\"Arguments: %s\", pformat(args))\n",
    "\n",
    "# Initialize distributed training if needed\n",
    "args.distributed = (args.local_rank != -1)\n",
    "if args.distributed:\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    args.device = torch.device(\"cuda\", args.local_rank)\n",
    "    torch.distributed.init_process_group(\n",
    "        backend='nccl', init_method='env://')\n",
    "\n",
    "logger.info(\"Prepare tokenizer, pretrained model and optimizer.\")\n",
    "# cant use Autotokenizer because checkpoint could be a Path\n",
    "tokenizer_class = GPT2Tokenizer if \"gpt2\" in args.model_checkpoint else OpenAIGPTTokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(args.model_checkpoint)\n",
    "\n",
    "model_class = GPT2DoubleHeadsModel if \"gpt2\" in args.model_checkpoint else OpenAIGPTDoubleHeadsModel\n",
    "model = model_class.from_pretrained(args.model_checkpoint)\n",
    "model.to(args.device)\n",
    "# Add special tokens if they are not already added\n",
    "add_special_tokens_(model, tokenizer)\n",
    "optimizer = AdamW(model.parameters(), lr=args.lr, correct_bias=True)\n",
    "\n",
    "# Prepare model for FP16 and distributed training if needed (order is important, distributed should be the last)\n",
    "if args.fp16:\n",
    "    from apex import amp  # Apex is only required if we use fp16 training\n",
    "    model, optimizer = amp.initialize(\n",
    "        model, optimizer, opt_level=args.fp16)\n",
    "if args.distributed:\n",
    "    model = DistributedDataParallel(\n",
    "        model, device_ids=[args.local_rank], output_device=args.local_rank)\n",
    "\n",
    "logger.info(\"Prepare datasets\")\n",
    "train_loader, val_loader, train_sampler, valid_sampler = get_data_loaders(\n",
    "    args, tokenizer)\n",
    "\n",
    "# Training function and trainer\n",
    "def update(engine, batch):\n",
    "    model.train()\n",
    "    batch = tuple(input_tensor.to(args.device) for input_tensor in batch)\n",
    "    input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids,cluster = batch\n",
    "    (lm_loss), (mc_loss), *_ = model(\n",
    "        input_ids, token_type_ids=token_type_ids, mc_token_ids=mc_token_ids,\n",
    "        mc_labels=mc_labels, lm_labels=lm_labels\n",
    "    )\n",
    "    loss = (lm_loss * args.lm_coef + mc_loss * args.mc_coef) / \\\n",
    "        args.gradient_accumulation_steps\n",
    "    if args.fp16:\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            amp.master_params(optimizer), args.max_norm)\n",
    "    else:\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_norm)\n",
    "    if engine.state.iteration % args.gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return loss.item()\n",
    "trainer = Engine(update)\n",
    "\n",
    "# Evaluation function and evaluator (evaluator output is the input of the metrics)\n",
    "def inference(engine, batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch = tuple(input_tensor.to(args.device)\n",
    "                        for input_tensor in batch)\n",
    "        input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids = batch\n",
    "        logger.info(tokenizer.decode(input_ids[0, -1, :].tolist()))\n",
    "        # if we dont send labels to model, it doesnt return losses\n",
    "        lm_logits, mc_logits, *_ = model(\n",
    "            input_ids, token_type_ids=token_type_ids, mc_token_ids=mc_token_ids,\n",
    "        )\n",
    "        lm_logits_flat_shifted = lm_logits[..., :-1,\n",
    "                                            :].contiguous().view(-1, lm_logits.size(-1))\n",
    "        lm_labels_flat_shifted = lm_labels[..., 1:].contiguous().view(-1)\n",
    "        return (lm_logits_flat_shifted, mc_logits), (lm_labels_flat_shifted, mc_labels)\n",
    "evaluator = Engine(inference)\n",
    "\n",
    "# Attach evaluation to trainer: we evaluate when we start the training and at the end of each epoch\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED,\n",
    "                            lambda _: evaluator.run(val_loader))\n",
    "if args.n_epochs < 1:\n",
    "    trainer.add_event_handler(\n",
    "        Events.COMPLETED, lambda _: evaluator.run(val_loader))\n",
    "if args.eval_before_start:\n",
    "    trainer.add_event_handler(\n",
    "        Events.STARTED, lambda _: evaluator.run(val_loader))\n",
    "\n",
    "# Make sure distributed data samplers split the dataset nicely between the distributed processes\n",
    "if args.distributed:\n",
    "    trainer.add_event_handler(\n",
    "        Events.EPOCH_STARTED, lambda engine: train_sampler.set_epoch(engine.state.epoch))\n",
    "    evaluator.add_event_handler(\n",
    "        Events.EPOCH_STARTED, lambda engine: valid_sampler.set_epoch(engine.state.epoch))\n",
    "\n",
    "# Linearly decrease the learning rate from lr to zero\n",
    "scheduler = PiecewiseLinear(\n",
    "    optimizer, \"lr\", [(0, args.lr), (args.n_epochs * len(train_loader), 0.0)])\n",
    "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
    "\n",
    "# Prepare metrics - note how we compute distributed metrics\n",
    "RunningAverage(output_transform=lambda x: x).attach(trainer, \"loss\")\n",
    "metrics = {\"nll\": Loss(torch.nn.CrossEntropyLoss(ignore_index=-100), output_transform=lambda x: (x[0][0], x[1][0])),\n",
    "            \"accuracy\": Accuracy(output_transform=lambda x: (x[0][1], x[1][1]))}\n",
    "metrics.update({\"average_nll\": MetricsLambda(average_distributed_scalar, metrics[\"nll\"], args),\n",
    "                \"average_accuracy\": MetricsLambda(average_distributed_scalar, metrics[\"accuracy\"], args)})\n",
    "metrics[\"average_ppl\"] = MetricsLambda(math.exp, metrics[\"average_nll\"])\n",
    "for name, metric in metrics.items():\n",
    "    metric.attach(evaluator, name)\n",
    "\n",
    "# On the main process: add progress bar, tensorboard, checkpoints and save model, configuration and tokenizer before we start to train\n",
    "if args.local_rank in [-1, 0]:\n",
    "    pbar = ProgressBar(persist=True)\n",
    "    pbar.attach(trainer, metric_names=[\"loss\"])\n",
    "    evaluator.add_event_handler(Events.COMPLETED, lambda _: pbar.log_message(\n",
    "        \"Validation: %s\" % pformat(evaluator.state.metrics)))\n",
    "\n",
    "    log_dir = make_logdir(args.model_checkpoint)\n",
    "    tb_logger = TensorboardLogger(log_dir)\n",
    "\n",
    "    tb_logger.attach(trainer, log_handler=OutputHandler(\n",
    "        tag=\"training\", metric_names=[\"loss\"]), event_name=Events.ITERATION_COMPLETED)\n",
    "    tb_logger.attach(trainer, log_handler=OptimizerParamsHandler(\n",
    "        optimizer), event_name=Events.ITERATION_STARTED)\n",
    "    tb_logger.attach(evaluator, log_handler=OutputHandler(tag=\"validation\", metric_names=list(\n",
    "        metrics.keys()), another_engine=trainer), event_name=Events.EPOCH_COMPLETED)\n",
    "\n",
    "    checkpoint_handler = ModelCheckpoint(\n",
    "        log_dir, 'checkpoint', save_interval=1, n_saved=3)\n",
    "    trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpoint_handler, {'mymodel': getattr(\n",
    "        model, 'module', model)})  # \"getattr\" takes care of distributed encapsulation\n",
    "\n",
    "    torch.save(args, log_dir + '/model_training_args.bin')\n",
    "    getattr(model, 'module', model).config.to_json_file(\n",
    "        os.path.join(log_dir, CONFIG_NAME))\n",
    "    tokenizer.save_pretrained(log_dir)\n",
    "\n",
    "# Run the training\n",
    "trainer.run(train_loader, max_epochs=args.n_epochs)\n",
    "\n",
    "# On the main process: close tensorboard logger and rename the last checkpoint (for easy re-loading with OpenAIGPTModel.from_pretrained method)\n",
    "if args.local_rank in [-1, 0] and args.n_epochs > 0:\n",
    "    # TODO: PR in ignite to have better access to saved file paths (cleaner)\n",
    "    os.rename(os.path.join(\n",
    "        log_dir, checkpoint_handler._saved[-1][1]), os.path.join(log_dir, WEIGHTS_NAME))\n",
    "    tb_logger.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0f6b48b0514f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtensor_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"valid\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TensorDataset(*tensor_datasets[\"valid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7801"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_datasets[\"valid\"][0].size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7801 7801\n",
      "tensor([[[50257,    72,  1100,  ..., 50262, 50262, 50262],\n",
      "         [50257,    72,  1100,  ..., 50262, 50262, 50262],\n",
      "         [50257,    72,  1100,  ..., 50262, 50262, 50262],\n",
      "         ...,\n",
      "         [50257,    72,  1100,  ..., 50262, 50262, 50262],\n",
      "         [50257,    72,  1100,  ..., 50262, 50262, 50262],\n",
      "         [50257,    72,  1100,  ..., 50262, 50262, 50262]],\n",
      "\n",
      "        [[50257,    72,  1100,  ..., 50262, 50262, 50262],\n",
      "         [50257,    72,  1100,  ..., 50262, 50262, 50262],\n",
      "         [50257,    72,  1100,  ..., 50262, 50262, 50262],\n",
      "         ...,\n",
      "         [50257,    72,  1100,  ..., 50262, 50262, 50262],\n",
      "         [50257,    72,  1100,  ..., 50262, 50262, 50262],\n",
      "         [50257,    72,  1100,  ..., 50262, 50262, 50262]],\n",
      "\n",
      "        [[50257,    72,  1100,  ..., 50262, 50262, 50262],\n",
      "         [50257,    72,  1100,  ..., 50262, 50262, 50262],\n",
      "         [50257,    72,  1100,  ..., 50262, 50262, 50262],\n",
      "         ...,\n",
      "         [50257,    72,  1100,  ..., 50262, 50262, 50262],\n",
      "         [50257,    72,  1100,  ..., 50262, 50262, 50262],\n",
      "         [50257,    72,  1100,  ..., 50262, 50262, 50262]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[50257,    72,   716,  ..., 50262, 50262, 50262],\n",
      "         [50257,    72,   716,  ..., 50262, 50262, 50262],\n",
      "         [50257,    72,   716,  ..., 50262, 50262, 50262],\n",
      "         ...,\n",
      "         [50257,    72,   716,  ..., 50262, 50262, 50262],\n",
      "         [50257,    72,   716,  ..., 50262, 50262, 50262],\n",
      "         [50257,    72,   716,  ..., 50262, 50262, 50262]],\n",
      "\n",
      "        [[50257,    72,   716,  ..., 50262, 50262, 50262],\n",
      "         [50257,    72,   716,  ..., 50262, 50262, 50262],\n",
      "         [50257,    72,   716,  ..., 50262, 50262, 50262],\n",
      "         ...,\n",
      "         [50257,    72,   716,  ..., 50262, 50262, 50262],\n",
      "         [50257,    72,   716,  ..., 50262, 50262, 50262],\n",
      "         [50257,    72,   716,  ..., 50262, 50262, 50262]],\n",
      "\n",
      "        [[50257,    72,   716,  ..., 50262, 50262, 50262],\n",
      "         [50257,    72,   716,  ..., 50262, 50262, 50262],\n",
      "         [50257,    72,   716,  ..., 50262, 50262, 50262],\n",
      "         ...,\n",
      "         [50257,    72,   716,  ..., 50262, 50262, 50262],\n",
      "         [50257,    72,   716,  ..., 50262, 50262, 50262],\n",
      "         [50257,    72,   716,  ..., 50262, 50262, 50262]]])\n",
      "7801 7801\n",
      "tensor([[ 62,  57,  60,  ...,  60,  55,  61],\n",
      "        [ 97,  89,  80,  ...,  80,  82,  84],\n",
      "        [106, 114, 120,  ..., 116, 117, 108],\n",
      "        ...,\n",
      "        [137, 129, 129,  ..., 139, 139, 133],\n",
      "        [134, 131, 124,  ..., 132, 129, 123],\n",
      "        [119, 116, 125,  ..., 123, 116, 119]])\n",
      "7801 7801\n",
      "tensor([[[-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         ...,\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100]],\n",
      "\n",
      "        [[-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         ...,\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100]],\n",
      "\n",
      "        [[-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         ...,\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         ...,\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100]],\n",
      "\n",
      "        [[-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         ...,\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100]],\n",
      "\n",
      "        [[-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         ...,\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100],\n",
      "         [-100, -100, -100,  ..., -100, -100, -100]]])\n",
      "7801 7801\n",
      "tensor([19, 19, 19,  ..., 19, 19, 19])\n",
      "7801 7801\n",
      "tensor([[[50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         ...,\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262]],\n",
      "\n",
      "        [[50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         ...,\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262]],\n",
      "\n",
      "        [[50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         ...,\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         ...,\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262]],\n",
      "\n",
      "        [[50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         ...,\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262]],\n",
      "\n",
      "        [[50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         ...,\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262],\n",
      "         [50260, 50260, 50260,  ..., 50262, 50262, 50262]]])\n",
      "7801 7801\n",
      "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "0 7801\n",
      "tensor([], size=(0, 20))\n"
     ]
    }
   ],
   "source": [
    "for t in tensor_datasets[\"valid\"]:\n",
    "    print(t.size(0),7801)\n",
    "    print(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tensor_datasets[\"valid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'valid'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "dataset = json.load(open(\"/home/xw_wangcs/transfer-learning-conv-ai/data/my_dataset_next_cluster.json\",'r'))#/personachat_self_original.json\",\"r\"))\n",
    "dataset.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset={}\n",
    "\n",
    "for k,v in dataset.items():\n",
    "    new_dataset[k] = []\n",
    "    for dialog in dataset[k]:\n",
    "        new_dialog = {}\n",
    "        new_dialog['utterances']=[]\n",
    "        for utter in  dialog['utterances']:\n",
    "            if utter['id']==2:\n",
    "                new_dialog['utterances'].append(utter)\n",
    "        new_dataset[k].append(new_dialog)\n",
    "import json\n",
    "json.dump(new_dataset,open(\"/home/xw_wangcs/transfer-learning-conv-ai/data/my_dataset_next_cluster_self.json\",'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "dataset = json.load(open(\"/home/xw_wangcs/transfer-learning-conv-ai/test.json\",'r'))#/personachat_self_original.json\",\"r\"))\n",
    "# dataset.keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "968"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "new_dataset = []\n",
    "for dialog in dataset[:5]:\n",
    "    new_utter = {}\n",
    "    new_utter['utterances'] = []\n",
    "    for utterance in dialog['utterances']:\n",
    "#         new_utter['utterances'].append(utterance)\n",
    "        for i in range(25):\n",
    "            tmp_utterance = copy.deepcopy(utterance)\n",
    "            tmp_utterance['cluster'] = i\n",
    "            new_utter['utterances'].append(tmp_utterance)\n",
    "    new_dataset.append(new_utter)\n",
    "    \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(new_dataset,open(\"/home/xw_wangcs/transfer-learning-conv-ai/data/my_dataset_test_switch_cluster.json\",'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
